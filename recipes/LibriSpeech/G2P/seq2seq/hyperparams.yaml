# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1234
__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]

# Data paths
output_folder: !ref results/RNN/<seed>
data_folder: /localscratch/LibriSpeech
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

input_lexicon: !ref <data_folder>/lexicon.csv
oov: !ref <data_folder>/oov.csv
wer_file: !ref <output_folder>/wer.txt

# These three files are created from lexicon.csv.
csv_train: !ref <data_folder>/lexicon_tr.csv
csv_valid: !ref <data_folder>/lexicon_dev.csv
csv_test: !ref <data_folder>/lexicon_test.csv


# Neural Parameters
N_epochs: 20
N_batch: 1024
lr: 0.002

# For DDP, use `python -m speechbrain.ddp experiment.py` to launch experiment
# These options will be set by that script, and will override these.
device: 'cuda:0'
multigpu_count: 0  # Set to number of GPUs if multi-gpu
multigpu_backend: null  # ['data_parallel', 'ddp_nccl', 'ddp_gloo', 'ddp_mpi']

# Model parameters
enc_dropout: 0.5
enc_neurons: 512
enc_num_layers: 4

dec_dropout: 0.5
dec_neurons: 512
dec_att_neurons: 256
dec_num_layers: 4

embedding_dim: 512

# Special Token infomation
bos: 40
eos: 40

# Data loaders
train_loader: !new:speechbrain.data_io.DataLoaderFactory
    csv_file: !ref <csv_train>
    csv_read: [graphemes, phonemes]
    batch_size: !ref <N_batch>
    sentence_sorting: ascending
    output_folder: !ref <output_folder>
    drop_last: False # Set it to True with multigpu_backend: data_parallel
    replacements:
        $data_folder: !ref <data_folder>

valid_loader: !new:speechbrain.data_io.DataLoaderFactory
    csv_file: !ref <csv_valid>
    csv_read: [graphemes, phonemes]
    batch_size: !ref <N_batch>
    sentence_sorting: ascending
    output_folder: !ref <output_folder>
    drop_last: False #  Set it to True with multigpu_backend: data_parallel
    replacements:
        $data_folder: !ref <data_folder>

test_loader: !new:speechbrain.data_io.DataLoaderFactory
    csv_file: !ref <csv_test>
    csv_read: [graphemes, phonemes]
    batch_size: !ref <N_batch>
    sentence_sorting: ascending
    output_folder: !ref <output_folder>
    replacements:
        $data_folder: !ref <data_folder>

# Models
enc: !new:speechbrain.nnet.LSTM
    input_shape: [null, null, !ref <embedding_dim>]
    bidirectional: True
    hidden_size: !ref <enc_neurons>
    num_layers: !ref <enc_num_layers>
    dropout: !ref <enc_dropout>

lin: !new:speechbrain.nnet.Linear
    input_size: !ref <dec_neurons>
    n_neurons: 41  # 39 phonemes + 1 eos
    bias: False

encoder_emb: !new:speechbrain.nnet.Embedding
    num_embeddings: 28  # 27 chars + 1 bos
    embedding_dim: !ref <embedding_dim>

emb: !new:speechbrain.nnet.Embedding
    num_embeddings: 41  # 39 phonemes + 1 bos
    embedding_dim: !ref <embedding_dim>

dec: !new:speechbrain.nnet.AttentionalRNNDecoder
    enc_dim: !ref <enc_neurons> * 2
    input_size: !ref <embedding_dim>
    rnn_type: gru
    attn_type: content
    dropout: !ref <dec_dropout>
    hidden_size: !ref <dec_neurons>
    attn_dim: !ref <dec_att_neurons>
    num_layers: !ref <dec_num_layers>

softmax: !new:speechbrain.nnet.Softmax
    apply_log: True

modules:
    enc: !ref <enc>
    encoder_emb: !ref <encoder_emb>
    emb: !ref <emb>
    dec: !ref <dec>
    lin: !ref <lin>

model: !new:torch.nn.ModuleList
    - [!ref <enc>, !ref <encoder_emb>, !ref <emb>, !ref <dec>, !ref <lin>]


opt_class: !name:torch.optim.Adam
    lr: !ref <lr>

beam_searcher: !new:speechbrain.decoders.S2SRNNBeamSearcher
    embedding: !ref <emb>
    decoder: !ref <dec>
    linear: !ref <lin>
    bos_index: !ref <bos>
    eos_index: !ref <eos>
    min_decode_ratio: 0
    max_decode_ratio: 1.35
    beam_size: 16
    eos_threshold: 10.0
    using_max_attn_shift: False
    max_attn_shift: 10
    coverage_penalty: 5.0


lr_annealing: !new:speechbrain.nnet.NewBobScheduler
    initial_value: !ref <lr>
    improvement_threshold: 0.0
    annealing_factor: 0.8
    patient: 0

compute_cost: !name:speechbrain.nnet.nll_loss

per_stats: !name:speechbrain.ErrorRateStats

per_computer: !name:speechbrain.ErrorRateStats
    split_tokens: True

epoch_counter: !new:speechbrain.EpochCounter
    limit: !ref <N_epochs>

train_logger: !new:speechbrain.FileTrainLogger
    save_file: !ref <train_log>

checkpointer: !new:speechbrain.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        scheduler: !ref <lr_annealing>
        counter: !ref <epoch_counter>
