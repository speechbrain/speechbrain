####################### Model parameters ###########################
save_folder: !PLACEHOLDER
num_codebooks: 8
sample_rate: 16000
codebook_size: 1024
block_size: 2048 
eos_token: !ref <codebook_size>
pad_token: !ref <codebook_size> + 1
vocab_size: !ref <codebook_size> + 2 # card(codebook) + eos + pad -> 1026
collapse_repeated_tokens: True

codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern
  audio_pad_token: !ref <pad_token>

model_backbone: !apply:transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: "HuggingFaceTB/SmolLM-135M"
  torch_dtype: !name:torch.bfloat16
  # attn_implementation: "flash_attention_2"
  cache_dir: "/scratch/adelmou/hf_home/hub/" 

config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig
  source: "HuggingFaceTB/SmolLM-135M"
  cache_dir: "/scratch/adelmou/hf_home/hub/"
  block_size: !ref <block_size>
  n_codebooks: !ref <num_codebooks>
  vocabsize: !ref <vocab_size>
  tie_embds: True

model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM
  config: !ref <config>
  model_backbone: !ref <model_backbone>

####################### Tokenizer parameters ###########################
tokenizer: !new:utils.tokenizer_interface.SpeechTokenizer
  source: fnlp/SpeechTokenizer  # Only the 24kHz version supports mono audio
  save_path: /scratch/adelmou/models/speech-tokenizer/SpeechTokenizer/

####################### Checkpointer parameters ###########################
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>

prompt_audio_file: /scratch/adelmou/datasets/sLM21-dataset/samples/semantic/dev/librispeech/oGlXErtmyp.wav
prompt_duration: -1
prefix_output: st