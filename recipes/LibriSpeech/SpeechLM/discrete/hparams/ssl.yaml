# Seed needs to be set at top of yaml, before objects with parameters are made
# TODO: add LinearScheduler ? Or Trapezoidal.
# TODO: add custom sampling logic.
seed: 1986
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
output_folder: !ref results/LibriSpeech/discrete_speechLM/wavlm/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER # e,g./path/to/LibriSpeech
tokens_folder: !PLACEHOLDER  # Path to the folder where extracted tokens are saved.
# data_folder_rirs: !ref <data_folder>
train_splits: ["dev-clean"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
train_csv: !ref <output_folder>/dev-clean.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv

# Training parameters
number_of_epochs: 20
lr: 3e-3
sorting: ascending
precision: bf16
skip_prep: False 
num_codebooks: 8 
sample_rate: 16000
codebook_size: 1024
block_size: 2048 
eos_token: !ref <codebook_size> + 1
pad_token: !ref <codebook_size> + 2
vocabsize: !ref <pad_token> + 1 # codebook_size + eos + pad -> 1026

# aim for something like 500k tokens / BP step.
# 32 * 2048 * 8 ~= 500k tokens
batch_size: 32
grad_accumulation_factor: 8
test_batch_size: 1
num_workers: 0

### discrete SSL configuration

# Dataloader options
train_dataloader_opts:
  shuffle: True
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>
# todo: limit the num of steps on val
valid_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>
test_dataloader_opts:
  batch_size: !ref <test_batch_size>
  # num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

### Config for Tokenizer
tokens_loader: !new:utils.tokens.TokensLoader
   data_path: !ref <tokens_folder>

codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern
  audio_pad_token: !ref <pad_token>

# define LM in the YAML
config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig
  source: "HuggingFaceTB/SmolLM-135M"
  cache_dir: "/scratch/adelmou/hf_home/hub/"
  block_size: !ref <block_size>
  n_codebooks: !ref <num_codebooks>
  vocabsize: !ref <vocabsize>
  tie_embds: True

model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM
  config: !ref <config>

lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
  initial_value: !ref <lr>
  final_value: 3e-5
  epoch_count: !ref <number_of_epochs>

opt_class: !name:torch.optim.AdamW
  lr: !ref <lr>
  betas: (0.9, 0.95)
   
modules:
  # codec: !ref <codec>
  model: !ref <model>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    counter: !ref <epoch_counter>
# add MFU.

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>  