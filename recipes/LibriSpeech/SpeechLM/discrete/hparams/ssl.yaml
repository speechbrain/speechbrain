# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
output_folder: !ref results/LibriSpeech/discrete_speechLM/wavlm/<seed>
save_folder: !ref <output_folder>/save

# Data files
data_folder: !PLACEHOLDER # e,g./path/to/LibriSpeech
tokens_folder: !PLACEHOLDER  # Path to the folder where extracted tokens are saved.
# data_folder_rirs: !ref <data_folder>
train_splits: ["dev-clean"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
train_csv: !ref <output_folder>/dev-clean.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv

# Training parameters
number_of_epochs: 20
lr: 0.0002
fine_weight: 1.0
sorting: ascending
precision: bf16
skip_prep: False 
num_codebooks: 8 
sample_rate: 16000
codebook_size: 1024
audio_pad_token: !ref <codebook_size> + 1 
# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 3 per GPU to fit 32GB of VRAM
batch_size: 4
test_batch_size: 1
num_workers: 5

### discrete SSL configuration

# Dataloader options
train_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <audio_pad_token>
# todo: limit the num of steps on val
valid_dataloader_opts:
  batch_size: !ref <batch_size>
  # num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <audio_pad_token>
test_dataloader_opts:
  batch_size: !ref <test_batch_size>
  # num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <audio_pad_token>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

### Config for Tokenizer
tokens_loader: !new:utils.tokens.TokensLoader
   data_path: !ref <tokens_folder>

codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern
  audio_pad_token: !ref <audio_pad_token>

block_size: 768 

config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig
  source: "HuggingFaceTB/SmolLM2-135M"
  cache_dir: "/scratch/adelmou/hf_home/hub/"
  block_size: !ref <block_size>
  n_codebooks: !ref <num_codebooks>
  vocabsize: 1001
  tie_embds: True

model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM
  config: !ref <config>

modules:
  # codec: !ref <codec>
  model: !ref <model>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    model: !ref <model>
    # codec: !ref <codec>
