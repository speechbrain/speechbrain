# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
output_folder: !ref results/LibriSpeech/discrete_speechLM/wavlm/<seed>
save_folder: !ref <output_folder>/save

# Data files
data_folder: !PLACEHOLDER # e,g./path/to/LibriSpeech
# noise/ris dataset will automatically be downloaded
# data_folder_rirs: !ref <data_folder>
train_splits: ["dev-clean"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
train_csv: !ref <output_folder>/dev-clean.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv

# Training parameters
number_of_epochs: 20
lr: 0.0002
fine_weight: 1.0
sorting: ascending
precision: bf16
skip_prep: False 

# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 3 per GPU to fit 32GB of VRAM
batch_size: 4
test_batch_size: 1

### discrete SSL configuration
ssl_hub: microsoft/wavlm-large
ssl_folder: !ref <save_folder>/ssl_checkpoint
kmeans_repo_id: speechbrain/SSL_Quantization
kmeans_cache_dir: !ref <save_folder>/kmeans_checkpoint
kmeans_dataset: LibriSpeech-100-360-500
freeze_ssl: True
freeze_feature_extractor: True
num_clusters: 1000

# Dataloader options
train_dataloader_opts:
  batch_size: !ref <batch_size>

valid_dataloader_opts:
  batch_size: !ref <batch_size>

test_dataloader_opts:
  batch_size: !ref <test_batch_size>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

### Config for Tokenizer
# Layer number should be among the supported layers for discrete SSL models(kmenas  model should be available for that layer)
# bpe_tokenizer_path: [null , null,  null, null]
ssl_layer_num: [1, 3, 7, 12, 18, 23]
num_codebooks: 6
deduplicate: [False, False, False, False, False, False]
bpe_tokenizer_path: [null, null, null, null, null, null]
sample_rate: 16000
encoder_dim: 1024
audio_pad_token: 0

tokenizer_config:
   SSL_layers: !ref <ssl_layer_num>
   deduplicates: !ref <deduplicate>
   bpe_tokenizers: !ref <bpe_tokenizer_path>

ssl_model: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
  source: !ref <ssl_hub>
  output_norm: False
  freeze: !ref <freeze_ssl>
  freeze_feature_extractor: !ref <freeze_feature_extractor>
  output_all_hiddens: True
  save_path: !ref <ssl_folder>
  
codec: !new:speechbrain.lobes.models.huggingface_transformers.discrete_ssl.DiscreteSSL
  save_path: !ref <kmeans_cache_dir>
  ssl_model: !ref <ssl_model>
  kmeans_dataset: !ref <kmeans_dataset>
  kmeans_repo_id: !ref <kmeans_repo_id>
  num_clusters: !ref <num_clusters>

codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern
  audio_pad_token: !ref <audio_pad_token>

block_size: 768 

config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig
  source: "HuggingFaceTB/SmolLM2-135M"
  cache_dir: "/scratch/adelmou/hf_home/hub/"
  block_size: !ref <block_size>
  n_codebooks: !ref <num_codebooks>
  vocabsize: 1001
  tie_embds: True

model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM
  config: !ref <config>

modules:
  codec: !ref <codec>
  model: !ref <model>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
    codec: !ref <codec>
