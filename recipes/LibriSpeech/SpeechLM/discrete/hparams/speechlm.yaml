# Seed needs to be set at top of yaml, before objects with parameters are made
# TODO: add LinearScheduler ? Or Trapezoidal.
# TODO: add custom sampling logic.
seed: 1986
__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]
experiment_name: hubert_overfit_dev_clean
output_folder: !ref results/LibriSpeech/<experiment_name>/<seed>
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: !PLACEHOLDER # e,g./path/to/LibriSpeech
tokens_folder: !PLACEHOLDER  # Path to the folder where extracted tokens are saved.
# data_folder_rirs: !ref <data_folder>
train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
# IMPORTANT: the CSVs files should be generated BEFORE when extracting the tokens.
csv_folder: /home/adelmou/proj/speechbrain/gslms/speechbrain/recipes/LibriSpeech/SpeechLM/discrete/results/LibriSpeech/discrete_speechLM/wavlm/1986
train_csv: !ref <csv_folder>/dev-clean.csv
valid_csv: !ref <csv_folder>/dev-clean.csv
test_csv:
   - !ref <csv_folder>/test-clean.csv
   - !ref <csv_folder>/test-other.csv

# Training parameters
number_of_epochs: 20
shuffle: False
lr: 3e-3
sorting: ascending
precision: bf16
eval_precision: bf16
skip_prep: False 
num_codebooks: 8 
sample_rate: 16000
codebook_size: 1024
block_size: 2048 
eos_token: !ref <codebook_size>
pad_token: !ref <codebook_size> + 1
vocabsize: !ref <codebook_size> + 2 # card(codebook) + eos + pad -> 1026
collapse_repeated_tokens: False

# aim for something like 1M tokens / BP step.
# 32 * 2048 * 16 ~= 1M tokens
batch_size: 32
grad_accumulation_factor: 8
test_batch_size: 1
num_workers: 0

### discrete SSL configuration

# Dataloader options
train_dataloader_opts:
  shuffle: !ref <shuffle>
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>
# todo: limit the num of steps on val
valid_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>
test_dataloader_opts:
  batch_size: !ref <test_batch_size>
  # num_workers: !ref <num_workers>
  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch
      padding_kwargs:
          value: !ref <pad_token>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

### Config for Tokenizer
tokens_loader: !new:utils.tokens.TokensLoader
   data_path: !ref <tokens_folder>

codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern
  audio_pad_token: !ref <pad_token>

model_backbone: !apply:transformers.AutoModelForCausalLM.from_pretrained
  pretrained_model_name_or_path: "HuggingFaceTB/SmolLM-135M"
  torch_dtype: !name:torch.bfloat16
  attn_implementation: "flash_attention_2"
  cache_dir: "/scratch/adelmou/hf_home/hub/" 

# define LM in the YAML
config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig
  source: "HuggingFaceTB/SmolLM-135M"
  cache_dir: "/scratch/adelmou/hf_home/hub/"
  block_size: !ref <block_size>
  n_codebooks: !ref <num_codebooks>
  vocabsize: !ref <vocabsize>
  tie_embds: True

model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM
  config: !ref <config>
  model_backbone: !ref <model_backbone>

lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler
  initial_value: !ref <lr>
  final_value: 3e-5
  epoch_count: !ref <number_of_epochs>

opt_class: !name:torch.optim.AdamW
  lr: !ref <lr>
  betas: (0.9, 0.95)
   
modules:
  # codec: !ref <codec>
  model: !ref <model>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <save_folder>
  recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>
  
wandb_logger: !new:speechbrain.utils.train_logger.WandBLogger
  entity: Zarko
  project: SLMs
  name: !ref <experiment_name> 
  reinit: True
  resume: False
  mode: "offline"
  dir: /home/adelmou/proj/speechbrain/gslms/speechbrain/recipes/LibriSpeech/SpeechLM/discrete/

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <train_log>  
