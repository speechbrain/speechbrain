{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adelmou/proj/speechbrain/gslms/speechbrain/recipes/LibriSpeech/SpeechLM/discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adelmou/gslms/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /home/adelmou/proj/speechbrain/gslms/speechbrain/recipes/LibriSpeech/SpeechLM/discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import logging\n",
    "import speechbrain as sb\n",
    "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from pathlib import Path\n",
    "from speechbrain.lobes.models.huggingface_transformers.discrete_speechlm import (\n",
    "    DiscreteSpeechLM,\n",
    "    DiscreteSpeechLMConfig,\n",
    "    InterleavedCodebookPattern,\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "class PackedDatasetWrapper(Dataset):\n",
    "    \"\"\"Wrapper that packs tokens from an existing DynamicItemDataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, original_dataset, block_size, token_key=\"tokens\", pad_token_id=-1):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.block_size = block_size\n",
    "        self.token_key = token_key\n",
    "        self.pad_token_id = pad_token_id\n",
    "        \n",
    "        # Precompute the mapping from block index to original data indices\n",
    "        self.blocks = []\n",
    "        self.blocks_ids = []\n",
    "        self._prepare_blocks()\n",
    "\n",
    "    def _prepare_blocks(self):\n",
    "        \"\"\"\n",
    "        Prepares the packed blocks by iterating through the original dataset,\n",
    "        concatenating tokens until reaching `block_size`, and handling padding.\n",
    "        \"\"\"\n",
    "        print(\"preparing blocks\")\n",
    "        # Generate list of indices\n",
    "        indices = list(range(len(self.original_dataset)))\n",
    "\n",
    "        buffer = []\n",
    "        buffer_length = 0\n",
    "        buffer_ids = []\n",
    "        print(self.original_dataset[0]['tokens'][:, 0])\n",
    "        for idx in indices:\n",
    "            data_point = self.original_dataset[idx]\n",
    "            tokens = data_point['tokens']  # Assuming 'tokens' is a list or tensor\n",
    "            id_name = data_point.get('id', idx)  # Optional: fetch an 'id' if available\n",
    "\n",
    "            # Convert tokens to tensor if they aren't already\n",
    "            if isinstance(tokens, list):\n",
    "                tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "            elif not isinstance(tokens, torch.Tensor):\n",
    "                raise ValueError(f\"Unsupported token type: {type(tokens)}\")\n",
    "\n",
    "            seq_length = tokens.size(0)\n",
    "            if buffer_length + seq_length > self.block_size:\n",
    "                if buffer:\n",
    "                    num_tokens_to_keep = self.block_size - buffer_length\n",
    "                    assert num_tokens_to_keep >= 0, num_tokens_to_keep\n",
    "                    new_tokens = tokens[:num_tokens_to_keep]\n",
    "                    buffer.append(new_tokens)\n",
    "                    buffer_ids.append(id_name)\n",
    "                    packed_tokens = torch.cat(buffer, dim=0)\n",
    "                    self.blocks.append(packed_tokens)\n",
    "                    self.blocks_ids.append(buffer_ids)\n",
    "                    \n",
    "                     # reset buffer\n",
    "                    buffer = []\n",
    "                    buffer_length = 0\n",
    "                    buffer_ids = []\n",
    "\n",
    "                    # calculate upper boundary of tokens to keep\n",
    "                    splitted_tokens = tokens[num_tokens_to_keep:].split(\n",
    "                        self.block_size\n",
    "                    )\n",
    "                    if len(splitted_tokens) > 0:\n",
    "                        for t in splitted_tokens:\n",
    "                            if t.shape[0] == self.block_size:\n",
    "                                buffer_ids.append(id_name)\n",
    "                                self.blocks.append(t)\n",
    "                                self.blocks_ids.append(buffer_ids)\n",
    "\n",
    "                    # Reset buffer and add the remaining tokens\n",
    "                    buffer = [splitted_tokens[-1]]\n",
    "                    buffer_ids = [id_name]\n",
    "                    buffer_length = splitted_tokens[-1].size(0)\n",
    "                else:\n",
    "                    # If single data point exceeds block_size, split it\n",
    "                    new_tokens = tokens[:self.block_size]\n",
    "                    buffer_ids.append(id_name)\n",
    "                    self.blocks.append(new_tokens)\n",
    "                    self.blocks_ids.append(buffer_ids)\n",
    "\n",
    "                    # reset buffer\n",
    "                    buffer = []\n",
    "                    buffer_length = 0\n",
    "                    buffer_ids = []\n",
    "\n",
    "                    # calculate upper boundary of tokens to keep\n",
    "                    splitted_tokens = tokens[self.block_size:].split(\n",
    "                        self.block_size\n",
    "                    )\n",
    "                    if len(splitted_tokens) > 0:\n",
    "                        for t in splitted_tokens:\n",
    "                            if t.shape[0] == self.block_size:\n",
    "                                buffer_ids.append(id_name)\n",
    "                                self.blocks.append(t)\n",
    "                                self.blocks_ids.append(buffer_ids)\n",
    "\n",
    "                    buffer = [splitted_tokens[-1]]\n",
    "                    buffer_ids = [id_name]\n",
    "                    buffer_length = splitted_tokens[-1].size(0) # - self.block_size\n",
    "            else:\n",
    "                buffer.append(tokens)\n",
    "                buffer_ids.append(id_name)\n",
    "                buffer_length += seq_length\n",
    "\n",
    "        # Handle remaining tokens in the buffer\n",
    "        if buffer:\n",
    "            concatenated = torch.cat(buffer, dim=0)\n",
    "            buffer_ids.append(id_name)\n",
    "            if buffer_length < self.block_size:\n",
    "                padding_length = self.block_size - buffer_length\n",
    "                padded_buffer = torch.cat([\n",
    "                    concatenated,\n",
    "                    torch.full((padding_length, concatenated.size(1)), self.pad_token_id, dtype=concatenated.dtype)\n",
    "                ], dim=0)\n",
    "                self.blocks.append(padded_buffer)\n",
    "                self.blocks_ids.append(buffer_ids)\n",
    "            else:\n",
    "                self.blocks.append(concatenated)\n",
    "                self.blocks_ids.append(buffer_ids)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        data_id = '-'.join(self.blocks_ids[idx])\n",
    "        data_point = self.blocks[idx]\n",
    "        return {\n",
    "            \"id\": data_id,\n",
    "            \"tokens\": data_point,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
    "    data_folder = \"test\"\n",
    "\n",
    "\n",
    "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
    "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
    "    )\n",
    "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
    "\n",
    "\n",
    "    datasets = [valid_data]\n",
    "\n",
    "    # 1. Define tokens pipeline:\n",
    "    tokens_loader = hparams[\"tokens_loader\"]\n",
    "    num_codebooks = hparams[\"num_codebooks\"]\n",
    "    \n",
    "    @sb.utils.data_pipeline.takes(\"id\")\n",
    "    @sb.utils.data_pipeline.provides(\"tokens\")\n",
    "    def tokens_pipeline(id):\n",
    "        # (T, C)\n",
    "        tokens = tokens_loader.tokens_by_uttid(id, num_codebooks=num_codebooks)\n",
    "        # concat eos_token to the end of the sequence\n",
    "        eos_token = torch.full((1, tokens.size(-1)), hparams['eos_token'], dtype=tokens.dtype, device=tokens.device)\n",
    "        tokens = torch.cat([tokens, eos_token], dim=0)\n",
    "        return tokens\n",
    "        \n",
    "    sb.dataio.dataset.add_dynamic_item(datasets, tokens_pipeline)\n",
    "\n",
    "    # 2. Set output:\n",
    "    sb.dataio.dataset.set_output_keys(\n",
    "        datasets, [\"id\", \"tokens\"],\n",
    "    )\n",
    "\n",
    "    valid_data = PackedDatasetWrapper(valid_data, 32, token_key=\"tokens\", pad_token_id=hparams[\"pad_token\"])\n",
    "    return valid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing blocks\n",
      "tensor([ 256,  907,  488,  153,    1,  478,  706,  170,  175,  859,  284,  324,\n",
      "         198,  726,  958,  330,  734,  942,  379,  640,  256,  256,  726,  369,\n",
      "         135,  890,  760,  241,   57,  894,  387,  829,  464,  984,  241,  441,\n",
      "         462,  965,  109,  242,  209,  241,  216,  324,  100,  771,  563,  635,\n",
      "        1017,  598,  178,  730,  750,  867,  324,  178,   32,  896,  484,  320,\n",
      "         256,  759,  163,  726,  151,  192,  958,  330,  175,  817,  367,  634,\n",
      "         949, 1024])\n",
      "tensor([[ 256,  907,  488,  153,    1,  478,  706,  170,  175,  859,  284,  324,\n",
      "          198,  726,  958,  330,  734,  942,  379,  640,  256,  256,  726,  369,\n",
      "          135,  890,  760,  241,   57,  894,  387,  829],\n",
      "        [ 464,  984,  241,  441,  462,  965,  109,  242,  209,  241,  216,  324,\n",
      "          100,  771,  563,  635, 1017,  598,  178,  730,  750,  867,  324,  178,\n",
      "           32,  896,  484,  320,  256,  759,  163,  726],\n",
      "        [ 151,  192,  958,  330,  175,  817,  367,  634,  949, 1024,  130,  832,\n",
      "          640,  726,  648,  167,  407,  949,  648,  379,  315,  379,    1,  958,\n",
      "          222,  379,    1,  473,  707,  473,    1,  167]])\n"
     ]
    }
   ],
   "source": [
    "valid_data = dataio_prepare(hparams)\n",
    "dataloader = sb.dataio.dataloader.make_dataloader(\n",
    "        valid_data,\n",
    "        batch_size=3,\n",
    "        num_workers=0,\n",
    "        collate_fn=lambda x: sb.dataio.batch.PaddedBatch(x, padding_kwargs={\"value\": hparams[\"pad_token\"]})\n",
    ")\n",
    "\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    tokens, _ = batch['tokens']\n",
    "    print(tokens[:, :, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems to work. We see the first item splitted into multiple batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.seed:[rank: 0] Setting seed to 1986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabsize =  1026\n"
     ]
    }
   ],
   "source": [
    "hparams = r\"\"\"\n",
    "seed: 1986\n",
    "__set_seed: !apply:speechbrain.utils.seed_everything [!ref <seed>]\n",
    "output_folder: !ref results/LibriSpeech/discrete_speechLM/wavlm/<seed>\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "# Data files\n",
    "tokens_folder: /scratch/adelmou/results/dac/librispeech\n",
    "csv_folder: /home/adelmou/proj/speechbrain/gslms/speechbrain/recipes/LibriSpeech/SpeechLM/discrete/results/LibriSpeech/discrete_speechLM/wavlm/1986\n",
    "train_csv: !ref <csv_folder>/train.csv\n",
    "valid_csv: !ref <csv_folder>/dev-clean.csv\n",
    "test_csv:\n",
    "   - !ref <csv_folder>/test-clean.csv\n",
    "   - !ref <csv_folder>/test-other.csv\n",
    "\n",
    "# Training parameters\n",
    "number_of_epochs: 20\n",
    "lr: 3e-3\n",
    "sorting: ascending\n",
    "precision: bf16\n",
    "skip_prep: True \n",
    "num_codebooks: 8 \n",
    "sample_rate: 16000\n",
    "codebook_size: 1024\n",
    "block_size: 2048 \n",
    "eos_token: !ref <codebook_size>\n",
    "pad_token: !ref <codebook_size> + 1\n",
    "vocabsize: !ref <codebook_size> + 2 # card(codebook) + eos + pad -> 1026\n",
    "\n",
    "# aim for something like 500k tokens / BP step.\n",
    "# 32 * 2048 * 8 ~= 500k tokens\n",
    "batch_size: 32\n",
    "grad_accumulation_factor: 8\n",
    "test_batch_size: 1\n",
    "num_workers: 0\n",
    "\n",
    "### discrete SSL configuration\n",
    "\n",
    "# Dataloader options\n",
    "train_dataloader_opts:\n",
    "  shuffle: True\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch\n",
    "      padding_kwargs:\n",
    "          value: !ref <pad_token>\n",
    "# todo: limit the num of steps on val\n",
    "valid_dataloader_opts:\n",
    "  batch_size: !ref <batch_size>\n",
    "  num_workers: !ref <num_workers>\n",
    "  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch\n",
    "      padding_kwargs:\n",
    "          value: !ref <pad_token>\n",
    "test_dataloader_opts:\n",
    "  batch_size: !ref <test_batch_size>\n",
    "  # num_workers: !ref <num_workers>\n",
    "  collate_fn: !name:speechbrain.dataio.batch.PaddedBatch\n",
    "      padding_kwargs:\n",
    "          value: !ref <pad_token>\n",
    "\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "   limit: !ref <number_of_epochs>\n",
    "\n",
    "### Config for Tokenizer\n",
    "tokens_loader: !new:utils.tokens.TokensLoader\n",
    "   data_path: !ref <tokens_folder>\n",
    "\n",
    "codebook_pattern: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.InterleavedCodebookPattern\n",
    "  audio_pad_token: !ref <pad_token>\n",
    "\n",
    "# define LM in the YAML\n",
    "config: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLMConfig\n",
    "  source: \"HuggingFaceTB/SmolLM-135M\"\n",
    "  cache_dir: \"/scratch/adelmou/hf_home/hub/\"\n",
    "  block_size: !ref <block_size>\n",
    "  n_codebooks: !ref <num_codebooks>\n",
    "  vocabsize: !ref <vocabsize>\n",
    "  tie_embds: True\n",
    "\n",
    "model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_speechlm.DiscreteSpeechLM\n",
    "  config: !ref <config>\n",
    "\n",
    "lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n",
    "  initial_value: !ref <lr>\n",
    "  final_value: 3e-5\n",
    "  epoch_count: !ref <number_of_epochs>\n",
    "\n",
    "opt_class: !name:torch.optim.AdamW\n",
    "  lr: !ref <lr>\n",
    "  betas: (0.9, 0.95)\n",
    "   \n",
    "modules:\n",
    "  model: !ref <model>\n",
    "\"\"\"\n",
    "hparams = load_hyperpyyaml(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare blocks\n"
     ]
    }
   ],
   "source": [
    "valid_data = dataio_prepare(hparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
