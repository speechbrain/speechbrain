# ############################################################################
# Model: LM for LibriSpeech
# Tokens: Pre-trained LibriSpeech tokens
# losses: NLL
# Training: LibriSpeech
# ############################################################################

save_folder: pretrained_models
lm_ckpt_file: https://www.dropbox.com/s/yqsod62r16lo5lu/lm_model.ckpt?dl=1
tokenizer_file: https://www.dropbox.com/s/j5u6e62tc9tl78a/tok_unigram.model?dl=1
device: 'cuda:0'

# Model params
d_model: 768
num_asr_tokens: 5000

model: !new:speechbrain.lobes.models.transformer.TransformerLM.TransformerLM # yamllint disable-line rule:line-length
    vocab: !ref <num_asr_tokens>
    d_model: !ref <d_model>
    nhead: 12
    num_encoder_layers: 12
    num_decoder_layers: 0
    d_ffn: 3072
    dropout: 0.0
    activation: !name:torch.nn.GELU
    normalize_before: False

tokenizer: !new:recipes.LibriSpeech.Tokenizer.pretrained.pretrained.tokenizer
    tokenizer_file: !ref <tokenizer_file>
    save_folder: !ref <save_folder>
