# ############################################################################
# Auido Tokenizer: WavLM
# Extraction: Librispeech 960h
# Authors: Jarod Duret 2024, Artem Ploujnikov 2025
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made

seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/wavlm
save_folder: !ref <output_folder>/save
storage_type: numpy
storage_opts:
  numpy:
    shard_size: !ref <storage_numpy_shard_size>
storage_numpy_shard_size: 1000
storage_numpy_async_save: False
storage_numpy_async_save_concurrency: 8

storage: !ref <storage_type>:<save_folder>/output?<storage_opts>
train_log: !ref <output_folder>/extraction_log.txt

# Data files
data_folder: !PLACEHOLDER  # e.g., /path/to/LibriSpeech
train_splits: ["train-clean-100"] #, "train-clean-360", "train-other-500"
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
skip_prep: False
train_json: !ref <output_folder>/train.json
valid_json: !ref <output_folder>/dev-clean.json
test_json: !ref <output_folder>/test.json
extract_features: ["audio_tokens"]

batch_size: 8
num_workers: 8
src_keys: ["id", "sig"]
id_key: id

# Dataloader options
dataloader_opts:
  batch_size: !ref <batch_size>
  shuffle: False
  num_workers: !ref <num_workers>

### Configuration for  discrete SSL model
# | SSL Model  | HF Encoder                             | K-Means Dataset | K-Means Size | SSL Layers           | Vocoder Model                            |
# |------------|----------------------------------------|-----------------|--------------|----------------------|------------------------------------------|
# | WavLM      | microsoft/wavlm-large                  | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | speechbrain/hifigan-wavlm-k1000-LibriTTS |
# | HuBERT     | facebook/hubert-large-ll60k            | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | WIP                                      |
# | Wav2Vec2   | facebook/wav2vec2-large-960h-lv60-self | LibriSpeech960  | 1000         | 1, 3, 7, 12, 18, 23  | WIP                                      |

# ssl_model_type: hubert, wavlm, wav2vec2
# ssl_hub: facebook/hubert-large-ll60k, microsoft/wavlm-large,  facebook/wav2vec2-large
ssl_model_type: wavlm
ssl_hub: microsoft/wavlm-large
pretrained_model_save_folder: !ref <save_folder>
ssl_folder: !ref <save_folder>/ssl_checkpoint
kmeans_cache_dir: !ref <save_folder>/kmeans_checkpoint
kmeans_dataset: LibriSpeech
vocoder_repo_id: speechbrain/hifigan-wavlm-k1000-LibriTTS
freeze_ssl: True
freeze_feature_extractor: True
vocab_size: 1000
save_embedding: False
skip_resample: False


### Config for Tokenizer
# Layer number should be among the supported layers for discrete SSL models(kmenas  model should be available for that layer)
num_codebooks: [1, 3, 7, 12, 18, 23]
deduplicate: null
sample_rate: 16000
encoder_dim: 1024

ssl_model: !apply:speechbrain.utils.hparams.choice
  value: !ref <ssl_model_type>
  choices:
    wavlm: !new:speechbrain.lobes.models.huggingface_transformers.wavlm.WavLM
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>
    hubert: !new:speechbrain.lobes.models.huggingface_transformers.hubert.HuBERT
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>
    wav2vec2: !new:speechbrain.lobes.models.huggingface_transformers.wav2vec2.Wav2Vec2
      source: !ref <ssl_hub>
      output_norm: False
      freeze: !ref <freeze_ssl>
      freeze_feature_extractor: !ref <freeze_feature_extractor>
      output_all_hiddens: True
      save_path: !ref <ssl_folder>

tokens_model: !new:speechbrain.lobes.models.huggingface_transformers.discrete_ssl.DiscreteSSL
  ssl_model: !ref <ssl_model>
  kmeans_dataset: !ref <kmeans_dataset>
  vocoder_repo_id: !ref <vocoder_repo_id>
  num_clusters: !ref <vocab_size>
  save_path: !ref <pretrained_model_save_folder>

modules:
  tokens_model: !ref <tokens_model>
  ssl_model: !ref <ssl_model>

tokenizer_kwargs:
  SSL_layers: !ref <num_codebooks>
  deduplicates: !ref <deduplicate>

feature_extractor: !name:speechbrain.dataio.preparation.FeatureExtractor
  storage: !ref <storage>
  storage_opts: !ref <storage_opts>
  src_keys: !ref <src_keys>
  id_key: !ref <id_key>
  dataloader_opts: !ref <dataloader_opts>
