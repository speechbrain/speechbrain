# ############################################################################
# Tokenizer: subword BPE with unigram 51
# Training: SLURP
# Authors:  Abdel Heba, Loren Lugosch 2021
# ############################################################################

output_folder: !ref results/tokenizer_bpe51/
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: /localscratch/SLURP
train_splits: ["train_synthetic", "train_real"]
train_csv: !ref <data_folder>/train-type=direct.csv
valid_csv: !ref <data_folder>/devel-type=direct.csv


# Training parameters
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 51  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
num_sequences: 10000
csv_read: semantics


tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   csv_train: !ref <train_csv>
   csv_read: !ref <csv_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   num_sequences: !ref <num_sequences>
   csv_list_to_check: [!ref <train_csv>, !ref <valid_csv>]
