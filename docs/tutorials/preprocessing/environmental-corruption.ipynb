{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_header",
    "tags": [
     "sb_auto_header"
    ]
   },
   "source": [
    "<!-- This cell is automatically updated by tools/tutorial-cell-updater.py -->\n",
    "<!-- The contents are initialized from tutorials/notebook-header.md -->\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/docs/tutorials/preprocessing/environmental-corruption.ipynb)\n",
    "to execute or view/download this notebook on\n",
    "[GitHub](https://github.com/speechbrain/speechbrain/tree/develop/docs/tutorials/preprocessing/environmental-corruption.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsPvUujzoDkw"
   },
   "source": [
    "# Environmental Corruption\n",
    "\n",
    "In realistic speech processing scenarios, the signals captured by microphones are often corrupted by unwanted elements such as **noise** and **reverberation**. This challenge is particularly pronounced in **distant-talking** (far-field) situations, where the speaker and the reference microphone are positioned at a considerable distance. Examples of such scenarios include signals recorded by popular devices like Google Home, Amazon Echo, Kinect, and similar devices.\n",
    "\n",
    "A common strategy in neural speech processing involves starting with clean speech recordings and artificially introducing noise and reverberation to simulate real-world conditions. This process is known as **environmental corruption** or *speech contamination*.\n",
    "\n",
    "Starting with clean signals allows for the controlled introduction of various types of noise and reverberation, making environmental corruption a potent regularization technique. This regularization helps neural networks generalize better when exposed to real-world, noisy conditions during testing.\n",
    "\n",
    "The environmental corruption process transforms a clean signal $x[n]$ into a noisy and reverberant signal using the equation:\n",
    "\n",
    "$y[n] = x[n] * h[n] + n[n]$\n",
    "\n",
    "where $n[n]$ represents a noise sequence, and $h[n]$ is an impulse response that introduces the reverberation effect.\n",
    "\n",
    "In the following sections, we will delve into the details of how this transformation is carried out. Before that, let's download some signals that will be essential for the rest of the tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZFaNUKuycE3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://www.dropbox.com/s/vwv8xdr7l3b2tta/noise_sig.csv\n",
    "!wget https://www.dropbox.com/s/aleer424jumcs08/noise2.wav\n",
    "!wget https://www.dropbox.com/s/eoxxi2ezr8owk8a/noise3.wav\n",
    "!wget https://www.dropbox.com/s/pjnub2s5hql2vxs/rir1.wav\n",
    "!wget https://www.dropbox.com/s/nyno6bqbmiy2rv8/rirs.csv\n",
    "!wget https://www.dropbox.com/s/u8qyvuyie2op286/spk1_snt1.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ildeW0Np9kkU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installing SpeechBrain\n",
    "BRANCH = \"develop\"\n",
    "!git clone https://github.com/speechbrain/speechbrain.git -b $BRANCH\n",
    "%cd /content/speechbrain/\n",
    "!python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQeluuEa2tqC"
   },
   "source": [
    "A clean speech signal looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZIni_cQ2tGm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from IPython.display import Audio\n",
    "\n",
    "clean = read_audio(\"/content/spk1_snt1.wav\").squeeze()\n",
    "\n",
    "# Plots\n",
    "plt.subplot(211)\n",
    "plt.plot(clean)\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.specgram(clean, Fs=16000)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "Audio(clean, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrljmzSm9LgK"
   },
   "source": [
    "## 1. Additive Noise\n",
    "\n",
    "In SpeechBrain, we designed a class able to contaminate a speech signal with noise (`speechbrain.augment.time_domanin.AddNoise`). This class takes in input a csv file that itemizes a list of noise signals:\n",
    "\n",
    "\n",
    "```\n",
    "ID, duration, wav, wav_format, wav_opts\n",
    "noise2, 5.0, noise2.wav, wav,\n",
    "noise3, 1.0, noise3.wav, wav,\n",
    "```\n",
    "When called, `AddNoise` samples from this noise collection and adds the selected noise into the clean signal with a random **Signal-to-Nose Ratio** (SNR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ji_LVoE5_LPK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from speechbrain.augment.time_domain import AddNoise\n",
    "\n",
    "noisifier = AddNoise(\n",
    "    \"tests/samples/annotation/noise.csv\",\n",
    "    replacements={\"noise_folder\": \"tests/samples/noise\"},\n",
    ")\n",
    "noisy = noisifier(clean.unsqueeze(0), torch.ones(1))\n",
    "\n",
    "# Plots\n",
    "plt.subplot(211)\n",
    "plt.plot(noisy.squeeze())\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.specgram(noisy.squeeze(), Fs=16000)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "Audio(noisy.squeeze(0), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsOSikxXB2zl"
   },
   "source": [
    "The amount of noise can be tuned with the **snr_low** and **snr_high** parameters that define the sampling range for the SNR. The length vector is needed because we can process in parallel batches of signals with different lengths. The length vector contains relative lengths for each sentence composing the batch (e.g, for two examples we can have lenght=[0.8 1.0] where 1.0 is the length of the longest sentence in the batch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkkgPahzDKLA"
   },
   "source": [
    "## 2. Reverberation\n",
    "When speaking into a room, our speech signal is **reflected multi-times** by the walls, floor, ceiling, and by the objects within the acoustic environment. Consequently, the final signal recorded by a distant microphone will contain multiple **delayed replicas** of the original signal. All these replicas interfere with each other and significantly affect the intelligibility of the speech signal.\n",
    "\n",
    "Such a **multi-path propagation** is called reverberation. Within a given room enclosure, the reverberation between a source and a receiver is modeled by an **impulse response**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylSShiytFv42"
   },
   "outputs": [],
   "source": [
    "rir = read_audio(\"/content/rir1.wav\")\n",
    "\n",
    "# Impulse response\n",
    "plt.subplot(211)\n",
    "plt.plot(rir[0:8000])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"h(t)\")\n",
    "\n",
    "# Zoom on early reflections\n",
    "plt.subplot(212)\n",
    "plt.plot(rir[2150:2500])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"h(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NirXcf9SGSEF"
   },
   "source": [
    "The impulse response is a complete description of the changes that the sounds undergo when traveling from a source to a receiver. In particular, each peak in the impulse response corresponds to a replica reaching the receiver. The first peak corresponds to the **direct path**. Then, we can see the **first-order reflections** on walls, ceiling, floor (see the second picture).\n",
    "\n",
    "Globally, the impulse response follows an exponential decay. This decay is faster in a dry room characterized by low reverberation-time and it is slower in a large and empty environment.\n",
    "\n",
    "The reverberation is added by performing a **convolution** between a clean signal and an impulse response. In SpeechBrain, this operation is performed by `speechbrain.processing.speech_augmentation.AddReverb`.\n",
    "\n",
    "When called, `AddRev` samples an impulse response from a given csv file:\n",
    "\n",
    "```\n",
    "ID, duration, wav, wav_format, wav_opts\n",
    "rir1, 1.0, rir1.wav, wav,\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd5PIl-AMUDf"
   },
   "outputs": [],
   "source": [
    "from speechbrain.augment.time_domain import AddReverb\n",
    "\n",
    "reverb = AddReverb(\n",
    "    \"tests/samples/annotation/RIRs.csv\",\n",
    "    replacements={\"rir_folder\": \"tests/samples/RIRs\"},\n",
    ")\n",
    "reverbed = reverb(clean)\n",
    "\n",
    "# Plots\n",
    "plt.subplot(211)\n",
    "plt.plot(reverbed.squeeze())\n",
    "plt.xlabel(\"Time\")\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.specgram(reverbed.squeeze(), Fs=16000)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "Audio(reverbed.squeeze(0), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhvVPlg_PrA7"
   },
   "source": [
    "Reverberation is a convolutive noise that \"smooths\" the signal in the time (see the long tails that appear in regions that were silent in the clean signal) and frequency domain.\n",
    "\n",
    "The amount of reverberation is controlled by the parameter **rir_scale_factor**. If rir_scale_factor < 1, the impulse response is compressed (less reverb), while if rir_scale_factor > 1 the impulse response is dilated (more reverb). Feel free to play with it in the previous example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9-_N33ElyqN"
   },
   "source": [
    "## References\n",
    "[1] M. Ravanelli, P. Svaizer, M. Omologo, \"Realistic Multi-Microphone Data Simulation for Distant Speech Recognition\",  in Proceedings of Interspeech 2016 [ArXiv](https://arxiv.org/abs/1711.09470)\n",
    "\n",
    "[2] M. Ravanelli, M. Omologo, \"Contaminated speech training methods for robust DNN-HMM distant speech recognition\", in Proceedings of  INTERSPEECH 2015. [ArXiv](https://arxiv.org/abs/1710.03538)\n",
    "\n",
    "[3] M. Ravanelli, M. Omologo, \"On the selection of the impulse responses for distant-speech recognition based on contaminated speech training\", in Proceedings of  INTERSPEECH 2014. [ArXiv](https://isca-speech.org/archive/archive_papers/interspeech_2014/i14_1028.pdf)\n",
    "\n",
    "[4] M. Ravanelli, A. Sosi, P. Svaizer, M.Omologo, \"Impulse response estimation for robust speech recognition in a reverberant environment\",   in Proceeding of the European Signal Processing Conference, EUSIPCO 2012. [ArXiv](https://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569588145.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_footer",
    "tags": [
     "sb_auto_footer"
    ]
   },
   "source": [
    "## Citing SpeechBrain\n",
    "\n",
    "If you use SpeechBrain in your research or business, please cite it using the following BibTeX entry:\n",
    "\n",
    "```bibtex\n",
    "@misc{speechbrainV1,\n",
    "  title={Open-Source Conversational AI with {SpeechBrain} 1.0},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve},\n",
    "  year={2024},\n",
    "  eprint={2407.00463},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={cs.LG},\n",
    "  url={https://arxiv.org/abs/2407.00463},\n",
    "}\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sIvjMP1xfgbyLTf6bjYmLVQ35a-BgxVd",
     "timestamp": 1612793552720
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
