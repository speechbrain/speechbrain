{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_header",
    "tags": [
     "sb_auto_header"
    ]
   },
   "source": [
    "<!-- This cell is automatically updated by tools/tutorial-cell-updater.py -->\n",
    "<!-- The contents are initialized from tutorials/notebook-header.md -->\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/docs/tutorials/preprocessing/multi-microphone-beamforming.ipynb)\n",
    "to execute or view/download this notebook on\n",
    "[GitHub](https://github.com/speechbrain/speechbrain/tree/develop/docs/tutorials/preprocessing/multi-microphone-beamforming.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5UgWOI8vYoC"
   },
   "source": [
    "# Multi-microphone Beamforming\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Using a microphone array can be very handy to improve the signal quality (e.g. reduce reverberation and noise) prior to performing speech recognition tasks.\n",
    "Microphone arrays can also estimate the direction of arrival of a sound source, and this information can later be used to \"listen\" in the direction of the source of interest.\n",
    "\n",
    "### Propagation model\n",
    "\n",
    "We assume the following propagation model for sound:\n",
    "\n",
    "$x_m[n] = h_m[n] \\star s[n] + b_m[n]$,\n",
    "\n",
    "where $m$ stands for the microphone index, $n$ for the sample index, and $h_m$ for the room impulse response. The expression $s[n]$ stands for the signal of the speech source, $b_m[n]$ the additive noise and $x_m[n]$ the signal captured at microphone $m$. The signals can also be expressed in the frequency domain:\n",
    "\n",
    "$X_m(t,j\\omega) = H_m(j\\omega)S(t,j\\omega) + B_m(t,j\\omega)$,\n",
    "\n",
    "or in the vector form:\n",
    "\n",
    "$\\mathbf{X}(t,j\\omega) = \\mathbf{H}(j\\omega)S(t,j\\omega) + \\mathbf{B}(t,j\\omega)$.\n",
    "\n",
    "Note that $\\mathbf{X}(t,j\\omega) \\in \\mathbb{C}^{M \\times 1}$.\n",
    "\n",
    "In the anechoic case, we can substitute $h_m[n] = a_m[n] = \\delta(n-\\tau_m)$, and we write $H_m(j\\omega) = A_m(j\\omega) = e^{-j\\omega\\tau_m}$, where $\\tau_m$ is the time delay for the direct path in samples, or in the vector form $\\mathbf{A}(j\\omega) \\in \\mathbb{C}^{M \\times 1}$.\n",
    "\n",
    "### Covariance matrices\n",
    "\n",
    "We also use the following covariance matrices with some beamformers:\n",
    "\n",
    "$\\displaystyle\\mathbf{R}_{XX}(j\\omega) = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{X}(t,j\\omega)\\mathbf{X}^H(t,j\\omega)$\n",
    "\n",
    "$\\displaystyle\\mathbf{R}_{SS}(j\\omega) = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{H}(j\\omega)\\mathbf{H}^H(j\\omega)|S(t,j\\omega)|^2$\n",
    "\n",
    "$\\displaystyle\\mathbf{R}_{NN}(j\\omega) = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{B}(t,j\\omega)\\mathbf{B}^H(t,j\\omega)$\n",
    "\n",
    "In practice, it is common to use an time-frequency mask to estimate the covariance matrices for speech and noise:\n",
    "\n",
    "$\\displaystyle\\mathbf{R}_{SS}(j\\omega) \\approx \\frac{1}{T}\\sum_{t=1}^{T}M_S(t,j\\omega)\\mathbf{X}(t,j\\omega)\\mathbf{X}^H(t,j\\omega)$\n",
    "\n",
    "$\\displaystyle\\mathbf{R}_{NN}(j\\omega) \\approx \\frac{1}{T}\\sum_{t=1}^{T}M_N(t,j\\omega)\\mathbf{X}(t,j\\omega)\\mathbf{X}^H(t,j\\omega)$\n",
    "\n",
    "### Time Difference of Arrival\n",
    "\n",
    "The time difference of arrival between microphone $1$ and $m$ can be estimated using the Generalized Cross-Correlation with Phase Transform (GCC-PHAT) with the following expression:\n",
    "\n",
    "$\\displaystyle\\tau_m = argmax_{\\tau} \\int_{-\\pi}^{+\\pi}{\\frac{X_1(j\\omega) X_m(j\\omega)^*}{|X_1(j\\omega)||X_m(j\\omega)|}e^{j\\omega\\tau}}d\\omega$\n",
    "\n",
    "### Direction of Arrival\n",
    "\n",
    "#### Steered-Response Power with Phase Transform\n",
    "\n",
    "SRP-PHAT scans each potential direction of arrival on a virtual unit sphere around the array and compute the corresponding power. For each DOA (denoted by the unit vector $\\mathbf{u}$), there is a steering vector $\\mathbf{A}(j\\omega,\\mathbf{u}) \\in \\mathbb{C}^{M \\times 1}$ in the direction of $\\mathbf{u}$:\n",
    "\n",
    "$\\displaystyle E(\\mathbf{u}) = \\sum_{p=1}^{M}{\\sum_{q=p+1}^{M}{\\int_{-\\pi}^{+\\pi}{\\frac{X_p(j\\omega)X_q(j\\omega)^*}{|X_p(j\\omega)||X_q(j\\omega)|}}}A_p(j\\omega,\\mathbf{u})A_q(j\\omega,\\mathbf{u})^* d\\omega}$\n",
    "\n",
    "The DOA with the maximum power is selected as the DOA of sound:\n",
    "\n",
    "$\\mathbf{u}_{max} = argmax_{\\mathbf{u}}{E(\\mathbf{u})}$\n",
    "\n",
    "#### Multiple Signal Classification\n",
    "\n",
    "MUSIC scans each potential direction of arrival on a virtual unit sphere around the array and compute the corresponding power. For each DOA (denoted by the unit vector $\\mathbf{u}$), there is a steering vector $\\mathbf{A}(j\\omega,\\mathbf{u}) \\in \\mathbb{C}^{M \\times 1}$ in the direction of $\\mathbf{u}$. The matrix $\\mathbf{U}(j\\omega) \\in \\mathbb{C}^{M \\times S}$ contains the $S$ eigenvectors that correspond to the $S$ smallest eigenvalues obtained while performing eigendecomposition on $\\mathbf{R}_{XX}(j\\omega)$. The power corresponds to:\n",
    "\n",
    "$\\displaystyle E(\\mathbf{u}) = \\frac{\\mathbf{A}(j\\omega,\\mathbf{u})^H \\mathbf{A}(j\\omega,\\mathbf{u})}{\\sqrt{\\mathbf{A}(j\\omega,\\mathbf{u})^H \\mathbf{U}(j\\omega)\\mathbf{U}(j\\omega)^H\\mathbf{A}(j\\omega,\\mathbf{u})}}$\n",
    "\n",
    "The DOA with the maximum power is selected as the DOA of sound:\n",
    "\n",
    "$\\mathbf{u}_{max} = argmax_{\\mathbf{u}}{E(\\mathbf{u})}$\n",
    "\n",
    "### Beamforming\n",
    "\n",
    "We apply beamforming in the frequency domain: $Y(j\\omega) = \\mathbf{W}^H(j\\omega)\\mathbf{X}(j\\omega)$.\n",
    "\n",
    "#### Delay and sum\n",
    "\n",
    "The delay and sum beamformer aims to align the speech signal to create constructive interference. The coefficients are chosen such that:\n",
    "\n",
    "$\\mathbf{W}(j\\omega) = \\frac{1}{M} \\mathbf{A}(j\\omega)$.\n",
    "\n",
    "#### Minimum Variance Distortionless Response\n",
    "\n",
    "The MVDR beamformer has the following coefficients:\n",
    "\n",
    "$\\displaystyle\\mathbf{W}(j\\omega) = \\frac{\\mathbf{R}_{XX}^{-1}(j\\omega)\\mathbf{A}(j\\omega)}{\\mathbf{A}^H(j\\omega)\\mathbf{R}_{XX}^{-1}(j\\omega)\\mathbf{A}(j\\omega)}$.\n",
    "\n",
    "#### Generalized Eigenvalue\n",
    "\n",
    "The GEV beamformer coefficients correspond to the principal component obtain from generalized eigenvalue decomposition, such that:\n",
    "\n",
    "$\\mathbf{R}_{SS}(j\\omega)\\mathbf{W}(j\\omega) = \\lambda\\mathbf{R}_{NN}(j\\omega)\\mathbf{W}(j\\omega)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH5Ko_p_1HT2"
   },
   "source": [
    "## Install SpeechBrain\n",
    "\n",
    "Let's first install SpeechBrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPcEjLmRvWs_"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installing SpeechBrain via pip\n",
    "BRANCH = \"develop\"\n",
    "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpMW386z87VF"
   },
   "source": [
    "## Prepare audio\n",
    "\n",
    "We will then load a speech signal obtained by simulating propagation in air for a 4-microphone array. We will also load diffuse noise (in all direction) and directive noise (can be modeled as a point source in space). The goal here is to mix the reverberated speech with noise to generate the noisy mixture, and test the beamforming methods to enhance speech.\n",
    "\n",
    "We first download the audio samples to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmIGszy4ovdm"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://www.dropbox.com/s/0h414xocvu9vw96/speech_-0.82918_0.55279_-0.082918.flac\n",
    "!wget https://www.dropbox.com/s/xlehxo26mnlkvln/noise_diffuse.flac\n",
    "!wget https://www.dropbox.com/s/4l6iy5zc9bgr7qj/noise_0.70225_-0.70225_0.11704.flac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNAQYH8NprTe"
   },
   "source": [
    "We will now load the audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nxAIphAp3z5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "\n",
    "xs_speech = read_audio(\"speech_-0.82918_0.55279_-0.082918.flac\")  # [time, channels]\n",
    "xs_speech = xs_speech.unsqueeze(0)  # [batch, time, channels]\n",
    "xs_noise_diff = read_audio(\"noise_diffuse.flac\")  # [time, channels]\n",
    "xs_noise_diff = xs_noise_diff.unsqueeze(0)  # [batch, time, channels]\n",
    "xs_noise_loc = read_audio(\"noise_0.70225_-0.70225_0.11704.flac\")  # [time, channels]\n",
    "xs_noise_loc = xs_noise_loc.unsqueeze(0)  # [batch, time, channels]\n",
    "fs = 16000  # sampling rate\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Clean signal at microphone 1\")\n",
    "plt.plot(xs_speech.squeeze()[:, 0])\n",
    "plt.figure(2)\n",
    "plt.title(\"Diffuse noise at microphone 1\")\n",
    "plt.plot(xs_noise_diff.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Directive noise at microphone 1\")\n",
    "plt.plot(xs_noise_loc.squeeze(0)[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D_V_zwMKxn_"
   },
   "source": [
    "We can listen to the reverberated speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQrxqNUDK7sB"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_speech.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAR6qZIeLosc"
   },
   "source": [
    "We now mix reverberated speech with noise to create the noisy multichannel mixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leHtExv6LkUg"
   },
   "outputs": [],
   "source": [
    "ss = xs_speech\n",
    "nn_diff = 0.05 * xs_noise_diff\n",
    "nn_loc = 0.05 * xs_noise_loc\n",
    "xs_diffused_noise = ss + nn_diff\n",
    "xs_localized_noise = ss + nn_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_rctgfSMapW"
   },
   "source": [
    "We can look at the noisy mixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJqEPdMUMZvt"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.title(\"Microphone 1 (speech + diffused noise)\")\n",
    "plt.plot(xs_diffused_noise.squeeze()[:, 0])\n",
    "plt.figure(2)\n",
    "plt.title(\"Microphone 1 (speech + directive noise)\")\n",
    "plt.plot(xs_localized_noise.squeeze()[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVNz-7hWLzZu"
   },
   "source": [
    "We can listen to the noisy mixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "315oSXLuL57U"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_diffused_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMkdjhyYM5KI"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_localized_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAokkFKbNKCa"
   },
   "source": [
    "## Processing\n",
    "\n",
    "### Steered-Response Power with Phase Transform\n",
    "\n",
    "STFT will convert the signals in the frequency domain, and then covariance will compute the covariance matrix for each frequency bin. The SRP-PHAT module will return the direction of arrival. We need to provide the microphone array geometry, which in this example is a circular array with four microphones uniformly spaced and a diameter of 0.1m. The system estimates the DOA for each STFT frame. In this example we use a sound source that comes from direction $x=-0.82918$, $y=0.55279$ and $z=-0.082918$. We see from the results that the direction is quite accurate (there is a slight difference due to the sphere discretization). Also note that as all microphones lie on the $xy$-plane, the system cannot distinguish from the positive $z$-axis and negative $z$-axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXuwwIwlBF2C"
   },
   "outputs": [],
   "source": [
    "from speechbrain.dataio.dataio import read_audio\n",
    "from speechbrain.processing.features import STFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import SrpPhat\n",
    "\n",
    "import torch\n",
    "\n",
    "mics = torch.zeros((4, 3), dtype=torch.float)\n",
    "mics[0, :] = torch.FloatTensor([-0.05, -0.05, +0.00])\n",
    "mics[1, :] = torch.FloatTensor([-0.05, +0.05, +0.00])\n",
    "mics[2, :] = torch.FloatTensor([+0.05, +0.05, +0.00])\n",
    "mics[3, :] = torch.FloatTensor([+0.05, +0.05, +0.00])\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "srpphat = SrpPhat(mics=mics)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "XXs = cov(Xs)\n",
    "doas = srpphat(XXs)\n",
    "\n",
    "print(doas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xau4s6U0Fih-"
   },
   "source": [
    "### Multiple Signal Classification\n",
    "\n",
    "STFT will convert the signals in the frequency domain, and then covariance will compute the covariance matrix for each frequency bin. The MUSIC module will return the direction of arrival. We need to provide the microphone array geometry, which in this example is a circular array with four microphones uniformly spaced and a diameter of 0.1m. The system estimates the DOA for each STFT frame. In this example we use a sound source that comes from direction $x=-0.82918$, $y=0.55279$ and $z=-0.082918$. We see from the results that the direction is quite accurate (there is a slight difference due to the sphere discretization). Also note that as all microphones lie on the $xy$-plane, the system cannot distinguish from the positive $z$-axis and negative $z$-axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug7Qub43Fq5r"
   },
   "outputs": [],
   "source": [
    "from speechbrain.dataio.dataio import read_audio\n",
    "from speechbrain.processing.features import STFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import Music\n",
    "\n",
    "import torch\n",
    "\n",
    "mics = torch.zeros((4, 3), dtype=torch.float)\n",
    "mics[0, :] = torch.FloatTensor([-0.05, -0.05, +0.00])\n",
    "mics[1, :] = torch.FloatTensor([-0.05, +0.05, +0.00])\n",
    "mics[2, :] = torch.FloatTensor([+0.05, +0.05, +0.00])\n",
    "mics[3, :] = torch.FloatTensor([+0.05, +0.05, +0.00])\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "music = Music(mics=mics)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "XXs = cov(Xs)\n",
    "doas = music(XXs)\n",
    "\n",
    "print(doas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbLs2iHNBlW9"
   },
   "source": [
    "\n",
    "\n",
    "### Delay-and-Sum Beamforming\n",
    "\n",
    "STFT will convert the signals in the frequency domain, and then covariance will compute the covariance matrix for each frequency bin. The GCC-PHAT module will estimate the Time Difference of Arrival (TDOA) between each microphone, and use this TDOA to perform delay and sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlDJWGQbUscv"
   },
   "source": [
    "#### Speech corrupted with diffuse noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxCgiowJNPup"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import GccPhat\n",
    "from speechbrain.processing.multi_mic import DelaySum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "delaysum = DelaySum()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "XXs = cov(Xs)\n",
    "tdoas = gccphat(XXs)\n",
    "Ys_ds = delaysum(Xs, tdoas)\n",
    "ys_ds = istft(Ys_ds)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_diffused_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_ds[0, :, :, 0, 0] ** 2 + Ys_ds[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_ds.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0toyEtcQ-NO"
   },
   "source": [
    "We can also listen to the beamformed signal and compare with the noisy signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Voy03xBQ_kJ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_diffused_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buI16SQ7Q_V7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_ds.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfV7jVPTSdqZ"
   },
   "source": [
    "#### Speech corrupted with directive noise\n",
    "\n",
    "When we have directive noise, this is more tricky as GCC-PHAT can capture the TDOAs from the noise source. For now we will simply assume we know the TDOAs, but ideal binary mask could be applied to differentiate the speech TDOAs from the noise TDOAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIW_9BszS1sW"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import GccPhat\n",
    "from speechbrain.processing.multi_mic import DelaySum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "delaysum = DelaySum()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "XXs = cov(Xs)\n",
    "tdoas = gccphat(XXs)\n",
    "\n",
    "Xs = stft(xs_localized_noise)\n",
    "XXs = cov(Xs)\n",
    "Ys_ds = delaysum(Xs, tdoas)\n",
    "ys_ds = istft(Ys_ds)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_diffused_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_ds[0, :, :, 0, 0] ** 2 + Ys_ds[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_ds.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn3H2PxdQj4u"
   },
   "source": [
    "We can also listen to the beamformed signal and compare with the noisy signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOQKGWVhQr26"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_localized_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIxw9lCjQzSV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_ds.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i88nwH4Q9qs"
   },
   "source": [
    "### Minimum Variance Distortionless Response\n",
    "\n",
    "STFT will convert the signals in the frequency domain, and then covariance will compute the covariance matrix for each frequency bin. The GCC-PHAT module will estimate the Time Difference of Arrival (TDOA) between each microphone, and use this TDOA to perform MVDR beamforming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbyyjssjUcCG"
   },
   "source": [
    "#### Speech corrupted with diffuse noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGwSJB65RegM"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import GccPhat\n",
    "from speechbrain.processing.multi_mic import Mvdr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "mvdr = Mvdr()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "Nn = stft(nn_diff)\n",
    "NNs = cov(Nn)\n",
    "XXs = cov(Xs)\n",
    "tdoas = gccphat(XXs)\n",
    "Ys_mvdr = mvdr(Xs, NNs, tdoas)\n",
    "ys_mvdr = istft(Ys_mvdr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_diffused_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_mvdr[0, :, :, 0, 0] ** 2 + Ys_mvdr[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_mvdr.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qf0vgNEKSSoG"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_diffused_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DivcNLlSWOW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_mvdr.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpD8JemOTH8f"
   },
   "source": [
    "#### Speech corrupted with directive noise\n",
    "Once again, when we have directive noise, this is more tricky as GCC-PHAT can capture the TDOAs from the noise source. For now we will simply assume we know the TDOAs, but ideal binary mask could be applied to differentiate the speech TDOAs from the noise TDOAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbYio7uGTUQ7"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import GccPhat\n",
    "from speechbrain.processing.multi_mic import Mvdr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "mvdr = Mvdr()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "Nn = stft(nn_loc)\n",
    "XXs = cov(Xs)\n",
    "NNs = cov(Nn)\n",
    "tdoas = gccphat(XXs)\n",
    "\n",
    "Xs = stft(xs_localized_noise)\n",
    "Ys_mvdr = mvdr(Xs, NNs, tdoas)\n",
    "ys_mvdr = istft(Ys_mvdr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_diffused_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_mvdr[0, :, :, 0, 0] ** 2 + Ys_mvdr[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_mvdr.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7LCNEA2Schg"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_localized_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3QdXk1NSoPy"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_mvdr.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSwWR_VOTnBf"
   },
   "source": [
    "### Generalized Eigenvalue Beamforming\n",
    "\n",
    "STFT will convert the signals in the frequency domain, and then covariance will compute the covariance matrix for each frequency bin. We assume we can compute the covariance matrix for speech and noise, respectively, and use it for beamforming. The covariance matrix can be estimated using ideal binary masks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EifFtuy3U2ak"
   },
   "source": [
    "#### Speech corrupted with diffuse noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_h1T92uU49p"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import Gev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "gev = Gev()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_diffused_noise)\n",
    "Ss = stft(ss)\n",
    "Nn = stft(nn_diff)\n",
    "SSs = cov(Ss)\n",
    "NNs = cov(Nn)\n",
    "Ys_gev = gev(Xs, SSs, NNs)\n",
    "ys_gev = istft(Ys_gev)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_localized_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_gev[0, :, :, 0, 0] ** 2 + Ys_gev[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_gev.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_hyaz0JVHD4"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_localized_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C42BoRZVKKR"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_gev.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKFqwp0nVA5J"
   },
   "source": [
    "#### Speech corrupted with directive noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Tha-6I7UER4"
   },
   "outputs": [],
   "source": [
    "from speechbrain.processing.features import STFT, ISTFT\n",
    "from speechbrain.processing.multi_mic import Covariance\n",
    "from speechbrain.processing.multi_mic import Gev\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "stft = STFT(sample_rate=fs)\n",
    "cov = Covariance()\n",
    "gccphat = GccPhat()\n",
    "gev = Gev()\n",
    "istft = ISTFT(sample_rate=fs)\n",
    "\n",
    "Xs = stft(xs_localized_noise)\n",
    "Ss = stft(ss)\n",
    "Nn = stft(nn_loc)\n",
    "SSs = cov(Ss)\n",
    "NNs = cov(Nn)\n",
    "Ys_gev = gev(Xs, SSs, NNs)\n",
    "ys_gev = istft(Ys_gev)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.imshow(\n",
    "    torch.transpose(torch.log(Xs[0, :, :, 0, 0] ** 2 + Xs[0, :, :, 1, 0] ** 2), 1, 0),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(2)\n",
    "plt.title(\"Noisy signal at microphone 1\")\n",
    "plt.plot(xs_localized_noise.squeeze()[:, 0])\n",
    "plt.figure(3)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.imshow(\n",
    "    torch.transpose(\n",
    "        torch.log(Ys_gev[0, :, :, 0, 0] ** 2 + Ys_gev[0, :, :, 1, 0] ** 2), 1, 0\n",
    "    ),\n",
    "    origin=\"lower\",\n",
    ")\n",
    "plt.figure(4)\n",
    "plt.title(\"Beamformed signal\")\n",
    "plt.plot(ys_gev.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwYFvqNPTtlJ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(xs_localized_noise.squeeze()[:, 0], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJXl4ZGJT5rz"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(ys_gev.squeeze(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_footer",
    "tags": [
     "sb_auto_footer"
    ]
   },
   "source": [
    "## Citing SpeechBrain\n",
    "\n",
    "If you use SpeechBrain in your research or business, please cite it using the following BibTeX entry:\n",
    "\n",
    "```bibtex\n",
    "@misc{speechbrainV1,\n",
    "  title={Open-Source Conversational AI with {SpeechBrain} 1.0},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve},\n",
    "  year={2024},\n",
    "  eprint={2407.00463},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={cs.LG},\n",
    "  url={https://arxiv.org/abs/2407.00463},\n",
    "}\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
