{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_header",
    "tags": [
     "sb_auto_header"
    ]
   },
   "source": [
    "<!-- This cell is automatically updated by tools/tutorial-cell-updater.py -->\n",
    "<!-- The contents are initialized from tutorials/notebook-header.md -->\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/docs/tutorials/advanced/inferring-on-your-own-speechbrain-models.ipynb)\n",
    "to execute or view/download this notebook on\n",
    "[GitHub](https://github.com/speechbrain/speechbrain/tree/develop/docs/tutorials/advanced/inferring-on-your-own-speechbrain-models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RTi_EwOXzfb"
   },
   "source": [
    "# Inferring on your trained SpeechBrain model\n",
    "\n",
    "In this tutorial, we will learn the different ways of inferring on a trained model. Please understand that this is not related to loading pretrained models for further training or transfer learning. If interested in these topics, refer to the corresponding [tutorial](https://colab.research.google.com/drive/1LN7R3U3xneDgDRK2gC5MzGkLysCWxuC3?usp=sharing).\n",
    "\n",
    "## Prerequisites\n",
    "- [SpeechBrain Introduction](https://speechbrain.readthedocs.io/en/latest/tutorials/basics/introduction-to-speechbrain.html)\n",
    "- [YAML tutorial](https://speechbrain.readthedocs.io/en/latest/tutorials/basics/hyperpyyaml.html)\n",
    "- [Brain Class tutorial](https://speechbrain.readthedocs.io/en/latest/tutorials/basics/brain-class.html)\n",
    "- [Pretraining tutorial](https://speechbrain.readthedocs.io/en/latest/tutorials/advanced/pre-trained-models-and-fine-tuning-with-huggingface.html\n",
    ")\n",
    "\n",
    "## Context\n",
    "\n",
    "In this example, we will consider a user that would like to use a custom pretrained speech recognizer **that has been trained by him** to transcribe some audio files. If you are interested in using online-available pretrained models, please refer to the [Pretraining tutorial](https://speechbrain.readthedocs.io/en/latest/tutorials/advanced/pre-trained-models-and-fine-tuning-with-huggingface.html\n",
    "). The following can be extended to any SpeechBrain supported task as we provide an homogeneous way of dealing with all of them.\n",
    "\n",
    "## Different options available\n",
    "\n",
    "At this point, three options are available to you:\n",
    "1. Define a custom python function in your ASR class (extended from Brain). This introduces strong coupling between the training recipe and your transcripts. It is pretty convenient for prototyping and obtaining simple transcripts on your datasets. However, it is not recommended for deployment.\n",
    "2. Use already available Interfaces (such as `EncoderDecoderASR`, introduction in the [pretraining tutorial](https://speechbrain.readthedocs.io/en/latest/tutorials/advanced/pre-trained-models-and-fine-tuning-with-huggingface.html\n",
    ")). This is probably the most elegant and convenient way. However, your model should be compliant with some constraints to fit the proposed interface.\n",
    "3. Build your own Interface perfectly fitting to your custom ASR model.\n",
    "\n",
    "**Important: All these solutions also apply to other tasks (speaker recognition, source separation ...)**\n",
    "\n",
    "### 1. Custom function in the training script\n",
    "The goal of this approach is to enable the user to call a function at the end of `train.py` that transcribes a given dataset:\n",
    "\n",
    "```python\n",
    "    # Trainer initialization\n",
    "    asr_brain = ASR(\n",
    "        modules=hparams[\"modules\"],\n",
    "        opt_class=hparams[\"opt_class\"],\n",
    "        hparams=hparams,\n",
    "        run_opts=run_opts,\n",
    "        checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    asr_brain.fit(\n",
    "        asr_brain.hparams.epoch_counter,\n",
    "        datasets[\"train\"],\n",
    "        datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"train_dataloader_opts\"],\n",
    "        valid_loader_kwargs=hparams[\"valid_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint for evaluation\n",
    "    test_stats = asr_brain.evaluate(\n",
    "        test_set=datasets[\"test\"],\n",
    "        min_key=\"WER\",\n",
    "        test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "    )\n",
    "\n",
    "    # Load best checkpoint for transcription !!!!!!\n",
    "    # You need to create this function w.r.t your system architecture !!!!!!\n",
    "    transcripts = asr_brain.transcribe_dataset(\n",
    "        dataset=datasets[\"your_dataset\"], # Must be obtained from the dataio_function\n",
    "        min_key=\"WER\", # We load the model with the lowest WER\n",
    "        loader_kwargs=hparams[\"transcribe_dataloader_opts\"], # opts for the dataloading\n",
    "    )\n",
    "```\n",
    "\n",
    "As you can see, there exists a strong coupling with the training recipe due to the need for an instantiated Brain class.\n",
    "\n",
    "**Note 1:** You can remove the `.fit()` and `.evaluate()` if you don't want to call them. This is just an example to better highlight how to use it.\n",
    "\n",
    "**Note 2:** Here, the `.transcribe_dataset()` function takes a `dataset` object to transcribe. You could also simply use a path instead. It is **completely** up to you to implement this function as you wish.\n",
    "\n",
    "Now: what to put in this function? Here, we will give an example based on the template, but you will need to adapt it to **your** system.\n",
    "\n",
    "```python\n",
    "\n",
    "def transcribe_dataset(\n",
    "        self,\n",
    "        dataset, # Must be obtained from the dataio_function\n",
    "        min_key, # We load the model with the lowest WER\n",
    "        loader_kwargs # opts for the dataloading\n",
    "    ):\n",
    "  \n",
    "    # If dataset isn't a Dataloader, we create it.\n",
    "    if not isinstance(dataset, DataLoader):\n",
    "        loader_kwargs[\"ckpt_prefix\"] = None\n",
    "        dataset = self.make_dataloader(\n",
    "            dataset, Stage.TEST, **loader_kwargs\n",
    "        )\n",
    "    \n",
    "    \n",
    "    self.on_evaluate_start(min_key=min_key) # We call the on_evaluate_start that will load the best model\n",
    "    self.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "\n",
    "        transcripts = []\n",
    "        for batch in tqdm(dataset, dynamic_ncols=True):\n",
    "            \n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied\n",
    "            # in compute_forward().\n",
    "            out = self.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            p_seq, wav_lens, predicted_tokens = out\n",
    "            \n",
    "            # We go from tokens to words.\n",
    "            predicted_words = self.tokenizer(\n",
    "                predicted_tokens, task=\"decode_from_list\"\n",
    "            )\n",
    "            transcripts.append(predicted_words)\n",
    "            \n",
    "    return transcripts\n",
    "```\n",
    "\n",
    "The pipeline is simple: load the model -> do compute_forward -> detokenize.\n",
    "\n",
    "### 2. Using the `EndoderDecoderASR` interface\n",
    "\n",
    "The [EncoderDecoderASR class](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/inference/ASR.py). interface allows you to decouple your trained model from the training recipe and to infer (or encode) on any new audio file in few lines of code. If you are not interested in ASR, you'll find many other interfaces to fit your purpose in the `interfaces.py` file. This solution must be preferred if you intend to deploy your model in a production fashion i.e. if you plan to use your model a lot and in a stable way. Of course, this will require you to slightly rework the yaml.\n",
    "\n",
    "The class has the following methods:\n",
    "\n",
    "- *encode_batch*: apply the encoder to an input batch and returns some encoded features.\n",
    "- *transcribe_file*: transcribes the single audio file in input.\n",
    "- *transcribe_batch*: transcribes the input batch.\n",
    "\n",
    "In fact, if you fulfill few constraints that we will detail in the next paragraph, you can simply do:\n",
    "\n",
    "```python\n",
    "from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(source=\"your_folder\", hparams_file='your_file.yaml', savedir=\"pretrained_model\")\n",
    "asr_model.transcribe_file('your_file.wav')\n",
    "```\n",
    "\n",
    "Nevertheless, to allow such a generalization over all the possible EncoderDecoder ASR pipelines, you will have to consider a few constraints when deploying your system:\n",
    "\n",
    "1. **Necessary modules.** As you can see in the `EncoderDecoderASR` class, the modules defined in your yaml file MUST contain certain elements with specific names. In practice, you need a tokenizer, a decoder, and a decoder. The encoder can simply be a `speechbrain.nnet.containers.LengthsCapableSequential` composed with a sequence of features computation, normalization and model encoding.\n",
    "```python\n",
    "    HPARAMS_NEEDED = [\"tokenizer\"]\n",
    "    MODULES_NEEDED = [\n",
    "        \"encoder\",\n",
    "        \"decoder\",\n",
    "    ]\n",
    "```\n",
    "\n",
    "You also need to declare these entities in the YAML file and create the following dictionary called `modules`:\n",
    "\n",
    "```yaml\n",
    "encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    compute_features: !ref <compute_features>\n",
    "    normalize: !ref <normalize>\n",
    "    model: !ref <enc>\n",
    "\n",
    "ctc_scorer: !new:speechbrain.decoders.scorer.CTCScorer\n",
    "    eos_index: !ref <eos_index>\n",
    "    blank_index: !ref <blank_index>\n",
    "    ctc_fc: !ref <ctc_lin>\n",
    "\n",
    "coverage_scorer: !new:speechbrain.decoders.scorer.CoverageScorer\n",
    "    vocab_size: !ref <output_neurons>\n",
    "\n",
    "rnnlm_scorer: !new:speechbrain.decoders.scorer.RNNLMScorer\n",
    "    language_model: !ref <lm_model>\n",
    "    temperature: !ref <temperature_lm>\n",
    "\n",
    "scorer: !new:speechbrain.decoders.scorer.ScorerBuilder\n",
    "    scorer_beam_scale: 1.5\n",
    "    full_scorers: [\n",
    "        !ref <rnnlm_scorer>,\n",
    "        !ref <coverage_scorer>]\n",
    "    partial_scorers: [!ref <ctc_scorer>]\n",
    "    weights:\n",
    "        rnnlm: !ref <lm_weight>\n",
    "        coverage: !ref <coverage_penalty>\n",
    "        ctc: !ref <ctc_weight_decode>\n",
    "\n",
    "decoder: !new:speechbrain.decoders.S2SRNNBeamSearcher\n",
    "    embedding: !ref <emb>\n",
    "    decoder: !ref <dec>\n",
    "    linear: !ref <seq_lin>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <test_beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    temperature: !ref <temperature>\n",
    "    scorer: !ref <scorer>\n",
    "\n",
    "modules:\n",
    "    encoder: !ref <encoder>\n",
    "    decoder: !ref <decoder>\n",
    "    lm_model: !ref <lm_model>\n",
    "```\n",
    "\n",
    "In this case, `enc` is a CRDNN, but could be any custom neural network for instance.\n",
    "\n",
    "  **Why do you need to ensure this?** Well, it simply is because these are the modules we call when inferring on the `EncoderDecoderASR` class. Here is an example of the `encode_batch()` function.\n",
    "```python\n",
    "[...]\n",
    "  wavs = wavs.float()\n",
    "  wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
    "  encoder_out = self.modules.encoder(wavs, wav_lens)\n",
    "return encoder_out\n",
    "```\n",
    "  **What if I have a complex asr_encoder structure with multiple deep neural networks and stuffs ?** Simply put everything in a torch.nn.ModuleList in your yaml:\n",
    "```yaml\n",
    "asr_encoder: !new:torch.nn.ModuleList\n",
    "    - [!ref <enc>, my_different_blocks ... ]\n",
    "```\n",
    "\n",
    "2. **Call to the pretrainer to load the checkpoints.** Finally, you need to define a call to the pretrainer that will load the different checkpoints of your trained model into the corresponding SpeechBrain modules. In short, it will load the weights of your encoder, language model or even simply load the tokenizer.\n",
    "```yaml\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        asr: !ref <asr_model>\n",
    "        lm: !ref <lm_model>\n",
    "        tokenizer: !ref <tokenizer>\n",
    "    paths:\n",
    "      asr: !ref <asr_model_ptfile>\n",
    "      lm: !ref <lm_model_ptfile>\n",
    "      tokenizer: !ref <tokenizer_ptfile>\n",
    "```\n",
    "The loadable field creates a link between a file (e.g. `lm` that is related to the checkpoint in `<lm_model_ptfile>`) to a yaml instance (e.g. `<lm_model>`) that is nothing more than your lm.\n",
    "\n",
    "If you respect these two constraints, it should works! Here, we give a complete example of a yaml that is used for inference only:\n",
    "\n",
    "```yaml\n",
    "\n",
    "# ############################################################################\n",
    "# Model: E2E ASR with attention-based ASR\n",
    "# Encoder: CRDNN model\n",
    "# Decoder: GRU + beamsearch + RNNLM\n",
    "# Tokens: BPE with unigram\n",
    "# Authors:  Ju-Chieh Chou, Mirco Ravanelli, Abdel Heba, Peter Plantinga 2020\n",
    "# ############################################################################\n",
    "\n",
    "\n",
    "# Feature parameters\n",
    "sample_rate: 16000\n",
    "n_fft: 400\n",
    "n_mels: 40\n",
    "\n",
    "# Model parameters\n",
    "activation: !name:torch.nn.LeakyReLU\n",
    "dropout: 0.15\n",
    "cnn_blocks: 2\n",
    "cnn_channels: (128, 256)\n",
    "inter_layer_pooling_size: (2, 2)\n",
    "cnn_kernelsize: (3, 3)\n",
    "time_pooling_size: 4\n",
    "rnn_class: !name:speechbrain.nnet.RNN.LSTM\n",
    "rnn_layers: 4\n",
    "rnn_neurons: 1024\n",
    "rnn_bidirectional: True\n",
    "dnn_blocks: 2\n",
    "dnn_neurons: 512\n",
    "emb_size: 128\n",
    "dec_neurons: 1024\n",
    "output_neurons: 1000  # index(blank/eos/bos) = 0\n",
    "blank_index: 0\n",
    "\n",
    "# Decoding parameters\n",
    "bos_index: 0\n",
    "eos_index: 0\n",
    "min_decode_ratio: 0.0\n",
    "max_decode_ratio: 1.0\n",
    "beam_size: 80\n",
    "eos_threshold: 1.5\n",
    "using_max_attn_shift: True\n",
    "max_attn_shift: 240\n",
    "lm_weight: 0.50\n",
    "coverage_penalty: 1.5\n",
    "temperature: 1.25\n",
    "temperature_lm: 1.25\n",
    "\n",
    "normalize: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: global\n",
    "\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    n_fft: !ref <n_fft>\n",
    "    n_mels: !ref <n_mels>\n",
    "\n",
    "enc: !new:speechbrain.lobes.models.CRDNN.CRDNN\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    activation: !ref <activation>\n",
    "    dropout: !ref <dropout>\n",
    "    cnn_blocks: !ref <cnn_blocks>\n",
    "    cnn_channels: !ref <cnn_channels>\n",
    "    cnn_kernelsize: !ref <cnn_kernelsize>\n",
    "    inter_layer_pooling_size: !ref <inter_layer_pooling_size>\n",
    "    time_pooling: True\n",
    "    using_2d_pooling: False\n",
    "    time_pooling_size: !ref <time_pooling_size>\n",
    "    rnn_class: !ref <rnn_class>\n",
    "    rnn_layers: !ref <rnn_layers>\n",
    "    rnn_neurons: !ref <rnn_neurons>\n",
    "    rnn_bidirectional: !ref <rnn_bidirectional>\n",
    "    rnn_re_init: True\n",
    "    dnn_blocks: !ref <dnn_blocks>\n",
    "    dnn_neurons: !ref <dnn_neurons>\n",
    "\n",
    "emb: !new:speechbrain.nnet.embedding.Embedding\n",
    "    num_embeddings: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "\n",
    "dec: !new:speechbrain.nnet.RNN.AttentionalRNNDecoder\n",
    "    enc_dim: !ref <dnn_neurons>\n",
    "    input_size: !ref <emb_size>\n",
    "    rnn_type: gru\n",
    "    attn_type: location\n",
    "    hidden_size: !ref <dec_neurons>\n",
    "    attn_dim: 1024\n",
    "    num_layers: 1\n",
    "    scaling: 1.0\n",
    "    channels: 10\n",
    "    kernel_size: 100\n",
    "    re_init: True\n",
    "    dropout: !ref <dropout>\n",
    "\n",
    "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dnn_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "seq_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <dec_neurons>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "log_softmax: !new:speechbrain.nnet.activations.Softmax\n",
    "    apply_log: True\n",
    "\n",
    "lm_model: !new:speechbrain.lobes.models.RNNLM.RNNLM\n",
    "    output_neurons: !ref <output_neurons>\n",
    "    embedding_dim: !ref <emb_size>\n",
    "    activation: !name:torch.nn.LeakyReLU\n",
    "    dropout: 0.0\n",
    "    rnn_layers: 2\n",
    "    rnn_neurons: 2048\n",
    "    dnn_blocks: 1\n",
    "    dnn_neurons: 512\n",
    "    return_hidden: True  # For inference\n",
    "\n",
    "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
    "\n",
    "asr_model: !new:torch.nn.ModuleList\n",
    "    - [!ref <enc>, !ref <emb>, !ref <dec>, !ref <ctc_lin>, !ref <seq_lin>]\n",
    "\n",
    "# We compose the inference (encoder) pipeline.\n",
    "encoder: !new:speechbrain.nnet.containers.LengthsCapableSequential\n",
    "    input_shape: [null, null, !ref <n_mels>]\n",
    "    compute_features: !ref <compute_features>\n",
    "    normalize: !ref <normalize>\n",
    "    model: !ref <enc>\n",
    "\n",
    "\n",
    "ctc_scorer: !new:speechbrain.decoders.scorer.CTCScorer\n",
    "    eos_index: !ref <eos_index>\n",
    "    blank_index: !ref <blank_index>\n",
    "    ctc_fc: !ref <ctc_lin>\n",
    "\n",
    "coverage_scorer: !new:speechbrain.decoders.scorer.CoverageScorer\n",
    "    vocab_size: !ref <output_neurons>\n",
    "\n",
    "rnnlm_scorer: !new:speechbrain.decoders.scorer.RNNLMScorer\n",
    "    language_model: !ref <lm_model>\n",
    "    temperature: !ref <temperature_lm>\n",
    "\n",
    "scorer: !new:speechbrain.decoders.scorer.ScorerBuilder\n",
    "    scorer_beam_scale: 1.5\n",
    "    full_scorers: [\n",
    "        !ref <rnnlm_scorer>,\n",
    "        !ref <coverage_scorer>]\n",
    "    partial_scorers: [!ref <ctc_scorer>]\n",
    "    weights:\n",
    "        rnnlm: !ref <lm_weight>\n",
    "        coverage: !ref <coverage_penalty>\n",
    "        ctc: !ref <ctc_weight_decode>\n",
    "\n",
    "decoder: !new:speechbrain.decoders.S2SRNNBeamSearcher\n",
    "    embedding: !ref <emb>\n",
    "    decoder: !ref <dec>\n",
    "    linear: !ref <seq_lin>\n",
    "    bos_index: !ref <bos_index>\n",
    "    eos_index: !ref <eos_index>\n",
    "    min_decode_ratio: !ref <min_decode_ratio>\n",
    "    max_decode_ratio: !ref <max_decode_ratio>\n",
    "    beam_size: !ref <test_beam_size>\n",
    "    eos_threshold: !ref <eos_threshold>\n",
    "    using_max_attn_shift: !ref <using_max_attn_shift>\n",
    "    max_attn_shift: !ref <max_attn_shift>\n",
    "    temperature: !ref <temperature>\n",
    "    scorer: !ref <scorer>\n",
    "    \n",
    "\n",
    "modules:\n",
    "    encoder: !ref <encoder>\n",
    "    decoder: !ref <decoder>\n",
    "    lm_model: !ref <lm_model>\n",
    "\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        asr: !ref <asr_model>\n",
    "        lm: !ref <lm_model>\n",
    "        tokenizer: !ref <tokenizer>\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, it is a standard YAMl file, but with a pretrainer that loads the model. It is similar to the yaml file used for training. We only have to remove all the parts that are training-specific (e.g, training parameters, optimizers, checkpointers, etc.) and add the pretrainer and `encoder`, `decoder` elements that links the needed modules with their pre-trained files.\n",
    "\n",
    "### 3. Developing your own inference interface\n",
    "\n",
    "While the `EncoderDecoderASR` class has been designed to be as generic as possible, your might require a more complex inference scheme that better fits your needs.  In this case, you have to develop your own interface. To do so, follow these steps:\n",
    "\n",
    "1. Create your custom interface inheriting from `Pretrained` (code [in this file](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/inference/interfaces.py)):\n",
    "\n",
    "\n",
    "```python\n",
    "class MySuperTask(Pretrained):\n",
    "  # Here, do not hesitate to also add some required modules\n",
    "  # for further transparency.\n",
    "  HPARAMS_NEEDED = [\"mymodule1\", \"mymodule2\"]\n",
    "  MODULES_NEEDED = [\n",
    "        \"mytask_enc\",\n",
    "        \"my_searcher\",\n",
    "  ]\n",
    "  def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Do whatever is needed here w.r.t your system\n",
    "```\n",
    "\n",
    "This will enable your class to call useful functions such as `.from_hparams()` that fetches and loads based on a HyperPyYAML file, `load_audio()` that loads a given audio file.  Likely, most of the methods that we coded in the Pretrained class will fit your need. If not, you can override them to implement your custom functionality.\n",
    "\n",
    "\n",
    "2. Develop your interface and the different functionalities. Unfortunately, we can't provide a generic enough example here. You can add **any** function to this class that you think can make inference on your data/model easier and natural. For instance, we can create here a function that simply encodes a wav file using the `mytask_enc` module.\n",
    "```python\n",
    "class MySuperTask(Pretrained):\n",
    "  # Here, do not hesitate to also add some required modules\n",
    "  # for further transparency.\n",
    "  HPARAMS_NEEDED = [\"mymodule1\", \"mymodule2\"]\n",
    "  MODULES_NEEDED = [\n",
    "        \"mytask_enc\",\n",
    "        \"my_searcher\",\n",
    "  ]\n",
    "  def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Do whatever is needed here w.r.t your system\n",
    "  \n",
    "  def encode_file(self, path):\n",
    "        waveform = self.load_audio(path)\n",
    "        # Fake a batch:\n",
    "        batch = waveform.unsqueeze(0)\n",
    "        rel_length = torch.tensor([1.0])\n",
    "        with torch.no_grad():\n",
    "          rel_lens = rel_length.to(self.device)\n",
    "          encoder_out = self.encode_batch(waveform, rel_lens)\n",
    "        \n",
    "        return encode_file\n",
    "```\n",
    "\n",
    "Now, we can use your Interface in the following way:\n",
    "```python\n",
    "from speechbrain.inference.my_super_task import MySuperTask\n",
    "\n",
    "my_model = MySuperTask.from_hparams(source=\"your_local_folder\", hparams_file='your_file.yaml', savedir=\"pretrained_model\")\n",
    "audio_file = 'your_file.wav'\n",
    "encoded = my_model.encode_file(audio_file)\n",
    "\n",
    "```\n",
    "\n",
    "As you can see, this formalism is extremely flexible and enables you to create a holistic interface that can be used to do anything you want with your pretrained model.\n",
    "\n",
    "We provide different generic interfaces for E2E ASR, speaker recognition, source separation, speech enhancement, etc. Please have a look [here](https://github.com/speechbrain/speechbrain/tree/develop/speechbrain/inference) if interested!\n",
    "\n",
    "\n",
    "## General Pretraining Inference\n",
    "In some cases, users might want to develop their inference interface in an external file. This can be done using the [foreign class](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/inference/interfaces.py).\n",
    "You can take a look at the example reported [here](https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "from speechbrain.inference.interfaces import foreign_class\n",
    "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\")\n",
    "out_prob, score, index, text_lab = classifier.classify_file(\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\")\n",
    "print(text_lab)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In this case, the inference interface is not a class written in `speechbrain.pretrained.interfaces`, but it is coded in an external file (`custom_interface.py`).\n",
    "\n",
    "This might be useful if the interface that you need is not available in `speechbrain.pretrained.interfaces`. If you want, you can add it there. If you use the foreign_class, however,  we also give you the possibility to fetch the inference code from any other path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb_auto_footer",
    "tags": [
     "sb_auto_footer"
    ]
   },
   "source": [
    "## Citing SpeechBrain\n",
    "\n",
    "If you use SpeechBrain in your research or business, please cite it using the following BibTeX entry:\n",
    "\n",
    "```bibtex\n",
    "@misc{speechbrainV1,\n",
    "  title={Open-Source Conversational AI with {SpeechBrain} 1.0},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Adel Moumen and Sylvain de Langen and Cem Subakan and Peter Plantinga and Yingzhi Wang and Pooneh Mousavi and Luca Della Libera and Artem Ploujnikov and Francesco Paissan and Davide Borra and Salah Zaiem and Zeyu Zhao and Shucong Zhang and Georgios Karakasidis and Sung-Lin Yeh and Pierre Champion and Aku Rouhe and Rudolf Braun and Florian Mai and Juan Zuluaga-Gomez and Seyed Mahed Mousavi and Andreas Nautsch and Xuechen Liu and Sangeet Sagar and Jarod Duret and Salima Mdhaffar and Gaelle Laperriere and Mickael Rouvier and Renato De Mori and Yannick Esteve},\n",
    "  year={2024},\n",
    "  eprint={2407.00463},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={cs.LG},\n",
    "  url={https://arxiv.org/abs/2407.00463},\n",
    "}\n",
    "@misc{speechbrain,\n",
    "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
    "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
    "  year={2021},\n",
    "  eprint={2106.04624},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={eess.AS},\n",
    "  note={arXiv:2106.04624}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
